{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56cf9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# general import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torch\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# increase cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc89735",
   "metadata": {},
   "source": [
    "# Pytorch main ingredient: tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d59265",
   "metadata": {},
   "source": [
    "Tensors are glorified numpy arrays: we go from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deaf84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numpy = np.array([0., 1., 2., 3., 4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587fae0",
   "metadata": {},
   "source": [
    "to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf5d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = torch.tensor([0., 1., 2., 3., 4.])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d4e0df",
   "metadata": {},
   "source": [
    "also, you can go back and forth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee188cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numpy_from_torch = X_torch.numpy()\n",
    "X_numpy_from_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f43f7",
   "metadata": {},
   "source": [
    "many functionalities carry over ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832be52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"min:\", X_torch.min().item())\n",
    "print(\"max:\", X_torch.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab825832",
   "metadata": {},
   "source": [
    "NOTE: I'm using item() since the result of a function on a tensor is also a tensor, indeed ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e89e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_torch.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f29277",
   "metadata": {},
   "source": [
    "This is useful when you store records during training of an ML model with backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050326ad",
   "metadata": {},
   "source": [
    "# Where magic happens: backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e879f9",
   "metadata": {},
   "source": [
    "Tensor can carry the information about a gradient of any function computed on them: all you have to do is\n",
    "set their requires_grad variable before computing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47176fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch.requires_grad = True # or equivalently run: X_torch.requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a77f2",
   "metadata": {},
   "source": [
    "Let's compute a simple (scalar) function of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (X_torch**2).sum()\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18406bac",
   "metadata": {},
   "source": [
    "The result has an associated grad_fn that can be used to compute the gradient wrt to its input variables.\n",
    "\n",
    "In this case: $\\frac{d f}{d x_i} = 2 x_i$ so we expect the gradient to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae23a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_grad = 2. * X_torch.detach()\n",
    "\n",
    "# WARNING: make sure to use detach() to perform a computation while not tracked by automatic differentiation\n",
    "# an equivalent approch would be:\n",
    "# while torch.no_grad():\n",
    "#     expected_grad = 2. * X_torch\n",
    "\n",
    "print(\"the old way of computing a gradient:\", expected_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b0b4a",
   "metadata": {},
   "source": [
    "Pytoch can do it for us for free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d19850c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward() # backpropagate and fill the grad variables in the tensor objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba1a3f",
   "metadata": {},
   "source": [
    "Let's inspect the grad variable in X_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ae99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the pytorch way of computing a gradient:\", X_torch.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2d091",
   "metadata": {},
   "source": [
    "In practice we will never actually access grad variables directly but use specific optimizers that can do the job for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45651ffa",
   "metadata": {},
   "source": [
    "# The simplest task: linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef975483",
   "metadata": {},
   "source": [
    "To learn how all this works, let's solve the simplest of problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ebaebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100 # number of input patterns\n",
    "N = 10 # number of parameters\n",
    "\n",
    "X = torch.randn(M, N) # input data\n",
    "y = torch.randn(M) # target data\n",
    "\n",
    "w = torch.randn(N).requires_grad_() # regression weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53e855",
   "metadata": {},
   "source": [
    "Let's us use gradient descent to solve this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3823c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1 # learning rate\n",
    "num_epochs = 100 # max number of gradient steps\n",
    "\n",
    "losses = []\n",
    "for ep in range(1, num_epochs+1):\n",
    "    y_pred = X @ w # compute predicted output\n",
    "    loss = ((y_pred - y)**2).mean() # compute loss\n",
    "    losses += [loss.item()] # record loss\n",
    "    \n",
    "    # zero grad\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    \n",
    "    # compute gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # apply gradient descent step\n",
    "    w.data -= lr * w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e65c05",
   "metadata": {},
   "source": [
    "Check the loss dynamics through learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c55146",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label=\"loss\");\n",
    "plt.title(\"loss through epochs\");\n",
    "plt.xlabel(\"ep\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14914c5a",
   "metadata": {},
   "source": [
    "Did we actually find the optimal w?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0276240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute optimal w from pseudo-inverse\n",
    "w_opt = torch.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "\n",
    "# compare with solution found by gradient descent\n",
    "plt.plot(w_opt.numpy(), w_opt.numpy(), ':', label=\"w opt\");\n",
    "plt.plot(w_opt.numpy(), w.detach().numpy(), '.', label=\"w\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fedbbd",
   "metadata": {},
   "source": [
    "# A network with one hidden layer: let's make sure torch works as it should"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce9f64",
   "metadata": {},
   "source": [
    "We'll check what pytorch can do on a simple network with one hidden layer trained on random input-output pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0a2834",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50 # input dimension\n",
    "M = 100 # number of input patterns\n",
    "\n",
    "X = torch.randn(M, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02820705",
   "metadata": {},
   "source": [
    "We'll use random {-1,+1} labels for simplicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e247c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2. * torch.randint(2, size=(M,)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91077c60",
   "metadata": {},
   "source": [
    "Initialize both layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5554f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 11 # number of neurons in the hidden layer\n",
    "\n",
    "W = torch.randn(N, K).requires_grad_() # first layer weights\n",
    "w = torch.randn(K).requires_grad_() # second layer (readout) weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53cceb7",
   "metadata": {},
   "source": [
    "For simplicity, we'll use a network with a linear output. Let's apply the layer to out inputs:\n",
    "\n",
    "$y_{pred}^\\mu = \\sum_{ = 1}^{K} w_k\\phi \\left(h^\\mu_k\\right)$\n",
    "\n",
    "where:\n",
    "\n",
    "$h^\\mu_k = \\sum_{i = 1}^{N} W_{ki} x_i^\\mu$\n",
    "\n",
    "and we called $\\phi$ the nonlinearity (in this case $\\tanh$).\n",
    "\n",
    "Make sure to note how I set the dimensions of the W tensor and why it's a transpose w.r.t. the previous formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7faf3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = X @ W # compute preactivation\n",
    "phi_h = torch.tanh(h) # pass preactivation through nonlinearity\n",
    "y_pred = phi_h @ w # compute output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862fa8d",
   "metadata": {},
   "source": [
    "Now, writing the loss as:\n",
    "\n",
    "$\\mathcal{L} = \\frac{1}{2 M} \\sum_{\\mu} \\left(y^\\mu - \\sum_k w_k \\tanh \\left(\\sum_i W_{ki} x_i^\\mu\\right) \\right)^2 $\n",
    "\n",
    "the true gradients are:\n",
    "\n",
    "$\\frac{ d \\mathcal{L}}{d W_{ki}} = \\frac{1}{M} \\sum_{\\mu} \\left(y_{pred}^\\mu - y^\\mu\\right) \\phi' \\left( h_k^\\mu \\right) w_k x_i^\\mu$\n",
    "\n",
    "$\\frac{ d \\mathcal{L}}{d w_{k}} = \\frac{1}{M} \\sum_{\\mu} \\left(y_{pred}^\\mu - y^\\mu\\right) \\tanh \\left(h_k^\\mu\\right)$\n",
    "\n",
    "We compute the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fd9a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = y_pred - y\n",
    "loss = 0.5 * (delta**2).mean()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dphi_h = 1. - phi_h**2 # derivative of tanh as a function of its output\n",
    "    grad_W_expected = (X.T @ (torch.outer(delta, w) * dphi_h)) / len(X) # gradient w.r.t W\n",
    "    grad_w_expected = (delta[:,None] * phi_h).mean(0) # gradient w.r.t 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e1ebe7",
   "metadata": {},
   "source": [
    "Let's check that pytorch computes the exact gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc39e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "\n",
    "grad_W_torch = W.grad\n",
    "grad_w_torch = w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f8c3562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad w is OK\n",
      "grad w is OK\n"
     ]
    }
   ],
   "source": [
    "yes_or_not_w = \"\" if torch.allclose(grad_w_torch, grad_w_expected) else \" not\"\n",
    "print(f\"grad w is{yes_or_not_w} OK\")\n",
    "\n",
    "yes_or_not_W = \"\" if torch.allclose(grad_W_torch, grad_W_expected) else \" not\"\n",
    "print(f\"grad w is{yes_or_not_W} OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c794b7",
   "metadata": {},
   "source": [
    "And lastly we make a final visual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba34fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(grad_w_expected.numpy().flatten(), grad_w_expected.numpy().flatten(), ':', label=\"expected\");\n",
    "plt.plot(grad_w_expected.numpy().flatten(), grad_w_torch.numpy().flatten(), '.', label=\"torch\");\n",
    "plt.legend();\n",
    "plt.title(\"w\");\n",
    "plt.xlabel(\"W\");\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(grad_W_expected.numpy().flatten(), grad_W_expected.numpy().flatten(), ':', label=\"expected\");\n",
    "plt.plot(grad_W_expected.numpy().flatten(), grad_W_torch.numpy().flatten(), '.', label=\"torch\");\n",
    "plt.legend();\n",
    "plt.title(\"W\");\n",
    "plt.xlabel(\"w\");\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d21f07",
   "metadata": {},
   "source": [
    "## Let's train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb3606",
   "metadata": {},
   "source": [
    "Compute output of untrained network first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5450b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = X @ W\n",
    "phi_h = torch.tanh(h)\n",
    "y_pred_init = phi_h @ w\n",
    "\n",
    "plt.hist(y_pred.detach()[y==-1], histtype=\"step\", label=\"y = -1\");\n",
    "plt.hist(y_pred.detach()[y==+1], histtype=\"step\", label=\"y = +1\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a5f7d",
   "metadata": {},
   "source": [
    "Now let's train this simple network using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2796478",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-1 # learning rate\n",
    "num_epochs = 5000 # max number of gradient steps\n",
    "\n",
    "losses = []\n",
    "for ep in range(1, num_epochs+1):\n",
    "    \n",
    "    # compute network output\n",
    "    h = X @ W\n",
    "    phi_h = torch.tanh(h)\n",
    "    y_pred = phi_h @ w\n",
    "    \n",
    "    # compute loss\n",
    "    delta = y_pred - y\n",
    "    loss = 0.5 * (delta**2).mean()\n",
    "    losses += [loss.item()]\n",
    "    \n",
    "    # zero grad\n",
    "    if W.grad is not None:\n",
    "        W.grad.zero_()\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "        \n",
    "    # compute gradient\n",
    "    loss.backward()\n",
    "    \n",
    "    # apply gradient descent step\n",
    "    W.data -= lr * W.grad\n",
    "    w.data -= lr * w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label=\"loss\");\n",
    "plt.title(\"loss through epochs\");\n",
    "plt.xlabel(\"ep\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d4604",
   "metadata": {},
   "source": [
    "Let's check what our network is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f159d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred.detach()[y==-1], histtype=\"step\", label=\"y = -1\");\n",
    "plt.hist(y_pred.detach()[y==+1], histtype=\"step\", label=\"y = +1\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7011a",
   "metadata": {},
   "source": [
    "# Where do we go from here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24936b02",
   "metadata": {},
   "source": [
    "1. It is best to wrap the network in a class;\n",
    "\n",
    "\n",
    "2. It is costumary to use pytorch dataloaders for inputs and targets. This is especially good for speed when you have acces to a GPU;\n",
    "\n",
    "\n",
    "3. Torch optimizer greatly simplify gradient updates.\n",
    "\n",
    "\n",
    "We will see how to write a training procedure in practice using these ingredients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
