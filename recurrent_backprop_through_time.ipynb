{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dadfd095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3892d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful utils we already used while training a FF network\n",
    "\n",
    "def get_dict(net, to_numpy = True):\n",
    "    if to_numpy:\n",
    "        return OrderedDict(\n",
    "            {k: v.detach().clone().to(\"cpu\").numpy() for k, v in net.state_dict().items()}\n",
    "        )\n",
    "    else:\n",
    "        return OrderedDict(\n",
    "            {k: v.detach().clone().to(\"cpu\") for k, v in net.state_dict().items()}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997ff12",
   "metadata": {},
   "source": [
    "# Let's wrap our network into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b585208",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    \"sigmoid\": torch.sigmoid,\n",
    "    \"tanh\":    torch.tanh,\n",
    "    \"relu\":    torch.relu,\n",
    "    \"linear\":  lambda x: x\n",
    "}\n",
    "\n",
    "def get_act(nonlinearity):\n",
    "    return activations[nonlinearity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81742a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, N = 10,\n",
    "                 g = 1.,\n",
    "                 num_in = 1,\n",
    "                 num_out = 1,\n",
    "                 nonlinearity = \"tanh\"):\n",
    "        \n",
    "        # init Module parent object\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # set parameters\n",
    "        self.N = N # number of neurons in the network\n",
    "        self.num_in = num_in # number of input units\n",
    "        self.num_out = num_out # number of output units\n",
    "        self.act = get_act(nonlinearity) # define activation function\n",
    "        \n",
    "        # init weights and biases -> wrap in torch Parameter to access torch.nn.Module functionalities\n",
    "        self.J = nn.Parameter(torch.randn(N, N) * g / np.sqrt(N)) # recurrent weights\n",
    "        self.w_in = nn.Parameter(torch.randn(num_in, N)) # input weights\n",
    "        self.w = nn.Parameter(torch.randn(N, num_out) / np.sqrt(N)) # output weights\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(N)) # single-neuron biases\n",
    "        self.bias_w = nn.Parameter(torch.zeros(num_out)) # output biases\n",
    "        \n",
    "    def forward(self, dt, inp, x0=None):\n",
    "        \n",
    "        # init records\n",
    "        titot, batch_size = inp.shape[:2]\n",
    "        xs = torch.zeros(titot, batch_size, self.N)\n",
    "        zs = torch.zeros(titot, batch_size, self.num_in)\n",
    "        \n",
    "        # init net\n",
    "        x = x0 if x0 is not None else torch.zeros(batch_size, self.N)\n",
    "        \n",
    "        # run\n",
    "        for ti in range(titot):\n",
    "            r = self.act(x)\n",
    "            x = (1. - dt) * x + dt * r @ self.J.T + dt * self.bias\n",
    "            x += dt * inp[ti] * self.w_in\n",
    "            z = r @ self.w + self.bias_w\n",
    "            xs[ti] = x\n",
    "            zs[ti] = z\n",
    "        \n",
    "        return xs, x, zs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1382e0f0",
   "metadata": {},
   "source": [
    "## Init network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a2f9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "g_init = 1.2\n",
    "num_in = 1\n",
    "num_out = 1\n",
    "nonlinearity = \"tanh\"\n",
    "\n",
    "net = Net(N=N,\n",
    "          g=g_init,\n",
    "          num_in=num_in,\n",
    "          num_out=num_out,\n",
    "          nonlinearity=nonlinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b98a9",
   "metadata": {},
   "source": [
    "## Test spontaneous activity in untrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72189e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run test\n",
    "dt = 0.1\n",
    "titot_test = 2000\n",
    "num_tests = 2\n",
    "inp = torch.zeros((titot_test, num_tests, num_in))\n",
    "\n",
    "with torch.no_grad():\n",
    "    x0 = torch.randn(num_tests, N)\n",
    "    xs, x, zs = net(dt, inp, x0=x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot records\n",
    "mu = 1 # note that we ran num_tests experiments all at once\n",
    "ts = np.arange(titot_test) * dt\n",
    "plt.plot(ts, xs[:,mu,:20], alpha=0.5);\n",
    "plt.plot(ts, zs[:,mu], color='black', label='z');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8aa65",
   "metadata": {},
   "source": [
    "# Set task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18b03c",
   "metadata": {},
   "source": [
    "We'll ask the network to produce a self-substained oscillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will ask the readout to reproduce an oscillation with a fixed frequency\n",
    "\n",
    "def target_func(t, freq = 0.05):\n",
    "    return torch.sin(2 * np.pi * freq * t) # WARNING: Here we're using torch.sin\n",
    "\n",
    "# take a look at the target function\n",
    "ts = torch.arange(0, 100, dt) # WARNING: Here we're using torch.arange\n",
    "plt.plot(ts, target_func(ts), '--', color=\"black\", label=\"target\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef86c2",
   "metadata": {},
   "source": [
    "## Let's write a test function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce4f57",
   "metadata": {},
   "source": [
    "When we're trying to train a stable dynamics the notion of generalization is the following:\n",
    "- generalization to unseen initial conditions x0\n",
    "- and stability to perturbations\n",
    "\n",
    "More generally, we will ask the network to generalize its response properties to unseen inputs, just like we saw in the feed-forward case with static images: this is what you will experiment in the Assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd29afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, zs_desired,\n",
    "         x0 = None,\n",
    "         num_tests = 1,\n",
    "         titot_test = 1,\n",
    "         dt = 1):\n",
    "        \n",
    "    err = 0\n",
    "    loss = 0.\n",
    "    inp = torch.zeros((titot_test, num_tests, num_in))\n",
    "    if x0 is None:\n",
    "        x0 = torch.randn(num_tests, N)\n",
    "    with torch.no_grad():\n",
    "        xs, x, zs = net(dt, inp, x0=x0)\n",
    "        err = ((zs[:,:,0] - zs_desired[:,None])**2).mean()\n",
    "    return xs, zs, err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f428e",
   "metadata": {},
   "source": [
    "### Test the test function (literally a meta test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = 10\n",
    "titot_test = 1000\n",
    "\n",
    "ts_test = torch.arange(titot_test) * dt\n",
    "zs_desired_test = target_func(ts_test)\n",
    "xs, zs_test, err_test = test(net, zs_desired_test, num_tests=10, titot_test=titot_test, dt=0.1)\n",
    "\n",
    "mu = 0\n",
    "plt.plot(ts_test, zs_desired_test, '--', color=\"black\", label=\"target\");\n",
    "plt.plot(ts_test, zs_test[:,mu,0], '-', color=\"red\", label=\"z\");\n",
    "\n",
    "print(\"test error:\", err_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252d1a3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a653ca",
   "metadata": {},
   "source": [
    "## set training parameters and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2aa2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "titot_train = 1000\n",
    "titot_test = 500\n",
    "\n",
    "batch_size = 1\n",
    "num_epochs = 1000\n",
    "test_every = 20\n",
    "# optimizer = 'SGD'\n",
    "optimizer = 'Adam'\n",
    "lr = 1e-3\n",
    "momentum = 0.\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823f4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train = net.parameters() # train all parameters\n",
    "\n",
    "if optimizer == 'SGD':\n",
    "    opt = torch.optim.SGD(to_train, lr=lr, momentum=momentum)\n",
    "elif optimizer == 'Adam':\n",
    "    opt = torch.optim.Adam(to_train, lr=lr)\n",
    "elif optimizer == 'RMSprop':\n",
    "    opt = torch.optim.RMSprop(to_train, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe579203",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb9773e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep things simple, let's only train with a given initial condition\n",
    "x0_start = 1e-1 * torch.randn(batch_size, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d14c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init containers and variables\n",
    "losses, losses_ep = [], []\n",
    "errs_train, errs_test, eps_test = [], [], []\n",
    "best_err_train, best_err_test = 1e10, 1e10\n",
    "best_net_train, best_net_test = None, None\n",
    "\n",
    "# utilities to measure time\n",
    "start_time = time.time()\n",
    "start_time_total = time.time()\n",
    "\n",
    "inp = torch.zeros((titot_train, batch_size, num_in))\n",
    "ts_train = ts_test = torch.arange(titot_train) * dt\n",
    "zs_desired_train = target_func(ts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fb07707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start testing right after training phase\n",
    "\n",
    "ts_test = ts_train[-1] + dt + torch.arange(titot_test) * dt\n",
    "zs_desired_test = target_func(ts_test)\n",
    "\n",
    "# # and that's how it looks like (make sure to comment the following lines after checking them out)\n",
    "# plt.plot(ts_train, zs_desired_train, label=\"train\");\n",
    "# plt.plot(ts_test, zs_desired_test, label=\"test\");\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "for ep in range(1, num_epochs+1):\n",
    " \n",
    "    x0 = x0_start + torch.randn_like(x0_start) * 1e-10 # check out what happens perturbing initial conditions\n",
    "    xs, x, zs = net(dt, inp, x0=x0)\n",
    "\n",
    "    # compute loss\n",
    "    loss = ((zs[:,:,0] - zs_desired_train[:,None])**2).mean()\n",
    "\n",
    "    # backpropagate and update weights\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # exit if NaN loss\n",
    "    if np.isnan(loss.item()):\n",
    "        raise ValueError(\"NaN happened\")\n",
    "\n",
    "    # record loss\n",
    "    losses += [loss.item()]\n",
    "\n",
    "    # compute err & test\n",
    "    err_test = np.nan\n",
    "    if ep % test_every == 0:\n",
    "        \n",
    "        # measure time and print loss\n",
    "        elapsed_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        eps_test += [ep]\n",
    "        toprint = 'ep: {} time: {:.2f} Loss: {:.4f}'.format(ep, elapsed_time/60, loss.item())\n",
    "\n",
    "        # check if improved train\n",
    "        improved_train = loss < best_err_train\n",
    "        if improved_train:\n",
    "            best_err_train = loss\n",
    "            best_net_train = get_dict(net)\n",
    "\n",
    "        # test network\n",
    "        xs_test, zs_test, err_test = test(net, zs_desired_test,\n",
    "                                          x0=x, # avoid a phase offset due to random initial conditions\n",
    "                                          num_tests=x.shape[0], titot_test=titot_test, dt=dt)\n",
    "        \n",
    "        toprint += ' g: {:.4f}'.format(err_test)\n",
    "        errs_test.append(err_test)\n",
    "        # check if improved test\n",
    "        improved_test = err_test < best_err_test\n",
    "        if improved_test:\n",
    "            best_err_test = err_test\n",
    "            best_net_test = get_dict(net)\n",
    "\n",
    "        # print if improved\n",
    "        if improved_train:\n",
    "            toprint += u' \\u2193'\n",
    "        if improved_test:\n",
    "            toprint += u' *'\n",
    "\n",
    "        print(toprint)\n",
    "        \n",
    "elapsed_time = time.time() - start_time_total\n",
    "print(\"done\")\n",
    "print(\"elapsed:\", str(datetime.timedelta(seconds=elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5213ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label=\"loss\");\n",
    "plt.plot(eps_test, errs_test, '.-', label=\"test err\");\n",
    "plt.xlabel(\"ep\");\n",
    "plt.ylabel(\"err\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d67e32",
   "metadata": {},
   "source": [
    "Let's take a look at the last training and test session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "plt.plot(ts_train, xs[:,mu,:20].detach(), alpha=0.2);\n",
    "plt.plot(ts_train, zs_desired_train, '--', color=\"blue\", label=\"target train\");\n",
    "plt.plot(ts_train, zs[:,mu,0].detach(), '-', color=\"purple\", label=\"z\");\n",
    "\n",
    "plt.vlines(x=ts_test[0], ymin=-3, ymax=3, lw=2, color='gray', ls=':');\n",
    "\n",
    "plt.plot(ts_test, xs_test[:,mu,:20].detach(), alpha=0.2);\n",
    "plt.plot(ts_test, zs_desired_test, '--', color=\"black\", label=\"target test\");\n",
    "plt.plot(ts_test, zs_test[:,mu,0], '-', color=\"red\", label=\"z\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e55dc",
   "metadata": {},
   "source": [
    "Try to test with random initial conditions now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tests = 1\n",
    "titot_test = 1000\n",
    "\n",
    "ts_test = torch.arange(titot_test) * dt\n",
    "zs_desired_test = target_func(ts_test)\n",
    "xs_test, zs_test, err_test = test(net, zs_desired_test, num_tests=10, titot_test=titot_test, dt=0.1)\n",
    "\n",
    "plt.plot(ts_test, zs_desired_test, '--', color=\"black\", label=\"target\");\n",
    "plt.plot(ts_test, zs_test[:,1,0], '-', color=\"red\", label=\"z\");\n",
    "\n",
    "# print(\"test error:\", err_test.item()) # test error doesn't really makes sense\n",
    "# ...unless one uses a matching algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51e9de",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59aace",
   "metadata": {},
   "source": [
    "1. Implement a sine with input-modulated frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efb068",
   "metadata": {},
   "source": [
    "2. OPTIONAL: Can you train a network composed of two distinct Excitatory and Inhibitory populations? Use a relu activation and enforce sign-constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92840ecb",
   "metadata": {},
   "source": [
    "3. ADVANCED: Recurrent networks can be used as classifiers. Can you set up the problem and train a network to classify MNIST digist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd290f5",
   "metadata": {},
   "source": [
    "## Some help with assignment 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94717a26",
   "metadata": {},
   "source": [
    "1. Use dataloaders (like we did in ff_bptt notebook) to feed random input-output batches to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711b46f",
   "metadata": {},
   "source": [
    "2. Ask the instructor for help for anything else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654087bf",
   "metadata": {},
   "source": [
    "## Some help with assignment 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc43cc",
   "metadata": {},
   "source": [
    "1. An EI separated initializiation is not enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15847b93",
   "metadata": {},
   "source": [
    "2. Start from a stable network by adjusting the stability of the (not necessarily zero) fixed point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08aa1f",
   "metadata": {},
   "source": [
    "## Some help with assignment 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c692da",
   "metadata": {},
   "source": [
    "Take some time to think about how an unrolled recurrent network is equivalent to a deep network with the same weights across layers ... in case of need, ask the instructor for help in real life or on Discord."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
